{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# GPUs config\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus) # Looks like we have one GPU, nice!\n",
    "# Config for GPU memory growth if needed\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42) # scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 5., 6.], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[1, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t) # matrix multiplication, same as tf.matmul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(t, tf.transpose(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[11., 26.],\n",
       "       [14., 35.],\n",
       "       [19., 46.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "K = keras.backend\n",
    "K.square(K.transpose(t)) + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy-TF compatibility\n",
    "import numpy as np\n",
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: add/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40) # Can't add int to float\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2] name: add/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40., dtype=tf.float64) # Can't add 32-bit to 64-bit\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hello world'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(b\"hello world\") # b is bytes, tf uses byte strings by default, if we pass in a unicode string like \"hello world\", it gets \n",
    "# encoded to UTF-8-byte-string ==> b'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hello world'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "bytes can only contain ASCII literal characters. (<ipython-input-24-d47c47917fd2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-d47c47917fd2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    tf.constant(b\"café\") # we can't cast the special e as one byte\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m bytes can only contain ASCII literal characters.\n"
     ]
    }
   ],
   "source": [
    "tf.constant(b\"café\") # we can't cast the special e as one byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(\"café\") # gets encoded to UTF-8-byte string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233])>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = tf.constant([ord(ch) for ch in \"café\"]) # ord returns an integer representing the Unicode character\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.strings.unicode_encode(u, \"UTF-8\") # encode to UTF-8-byte string\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=4>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.length(b, unit=\"UTF8_CHAR\") # A UTF-8-byte string (or in general, a tf.string) is atomic, i.e., its length won't\n",
    "# show in the tensor's shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233])>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_decode(b, \"UTF-8\") # After decoding to integer representation, the length will show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
       "array([b'Caf\\xc3\\xa9', b'Coffee', b'caff\\xc3\\xa8',\n",
       "       b'\\xe5\\x92\\x96\\xe5\\x95\\xa1'], dtype=object)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"]) # Unicode to UTF-8\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2])>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.length(p, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = tf.strings.unicode_decode(p, \"UTF8\") # decoding gives integer representations of unicode chracters. UTF-8 to Unicode\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]>\n"
     ]
    }
   ],
   "source": [
    "print(r) #  it is a tensor with one or more ragged dimensions, meaning dimensions whose slices may have different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 67 111 102 102 101 101], shape=(6,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232]]>\n"
     ]
    }
   ],
   "source": [
    "print(r[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857], [65, 66], [], [67]]>\n"
     ]
    }
   ],
   "source": [
    "r2 = tf.ragged.constant([[65, 66], [], [67]])\n",
    "print(tf.concat([r, r2], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[67, 97, 102, 233, 68, 69, 70], [67, 111, 102, 102, 101, 101, 71], [99, 97, 102, 102, 232], [21654, 21857, 72, 73]]>\n"
     ]
    }
   ],
   "source": [
    "r3 = tf.ragged.constant([[68, 69, 70], [71], [], [72, 73]])\n",
    "print(tf.concat([r, r3], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=array([b'DEF', b'G', b'', b'HI'], dtype=object)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_encode(r3, \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 6), dtype=int32, numpy=\n",
       "array([[   67,    97,   102,   233,     0,     0],\n",
       "       [   67,   111,   102,   102,   101,   101],\n",
       "       [   99,    97,   102,   102,   232,     0],\n",
       "       [21654, 21857,     0,     0,     0,     0]])>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.SparseTensor(indices=[[0, 1], [1, 0], [2, 3]],\n",
    "                    values=[1., 2., 3.],\n",
    "                    dense_shape=[3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [2 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [2., 0., 0., 0.],\n",
       "       [0., 0., 0., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s * 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupported operand type(s) for +: 'SparseTensor' and 'int'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s3 = s + 1\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[ 30.,  40.],\n",
       "       [ 20.,  40.],\n",
       "       [210., 240.]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4 = tf.constant([[10., 20.], [30., 40.], [50., 60.], [70., 80.]])\n",
    "tf.sparse.sparse_dense_matmul(s, s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 2]\n",
      " [0 1]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "s5 = tf.SparseTensor(indices=[[0, 2], [0, 1]], # Must be in reading order (left to right, up to down)\n",
    "                     values=[1., 2.],\n",
    "                     dense_shape=[3, 4])\n",
    "print(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices[1] = [0,1] is out of order. Many sparse ops require sorted indices.\n",
      "    Use `tf.sparse.reorder` to create a correctly ordered copy.\n",
      "\n",
      " [Op:SparseToDense]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.sparse.to_dense(s5)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 2., 1., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s6 = tf.sparse.reorder(s5)\n",
    "tf.sparse.to_dense(s6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[ 2,  3,  4,  5,  6,  7],\n",
       "       [ 0,  7,  9, 10,  0,  0]])>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1 = tf.constant([[2, 3, 5, 7], [7, 9, 0, 0]])\n",
    "set2 = tf.constant([[4, 5, 6], [9, 10, 0]])\n",
    "tf.sparse.to_dense(tf.sets.union(set1, set2)) # Note the last two zeroes are padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[ 2,  3,  4,  5,  6,  7],\n",
       "       [ 0,  7,  9, 10, -1, -1]])>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(tf.sets.union(set1, set2), default_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[ 2,  3,  7],\n",
       "       [ 7, -1, -1]])>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(tf.sets.difference(set1, set2), default_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[ 4,  6],\n",
       "       [10, -1]])>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(tf.sets.difference(set2, set1), default_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[ 5, -1],\n",
       "       [ 0,  9]])>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(tf.sets.intersection(set1, set2), default_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v) # Note that it's done in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ResourceVariable' object does not support item assignment\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    v[0, 1] = 43 # Must use assign\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]],\n",
    "                    updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[4., 5., 6.],\n",
       "       [1., 2., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],\n",
    "                                indices=[1, 0])\n",
    "v.scatter_update(sparse_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[4., 5., 6.],\n",
       "       [1., 2., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = tf.TensorArray(dtype=tf.float32, size=3)\n",
    "array = array.write(0, tf.constant([1., 2.])) # Must assign back to array otherwise Eager mode would work but not Graph mode\n",
    "array = array.write(1, tf.constant([3., 10.]))\n",
    "array = array.write(2, tf.constant([5., 7.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 3., 10.], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.read(1) # This will clear index 1 as demonstrated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Could not read index 1 twice because it was cleared after a previous read (perhaps try setting clear_after_read = false?)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-6bd69ee861b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\taij\\documents\\calvin\\myprojects\\envcpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\tensor_array_ops.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, index, name)\u001b[0m\n\u001b[0;32m   1135\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mat\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m     \"\"\"\n\u001b[1;32m-> 1137\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_implementation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mtf_should_use\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_use_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwarn_in_eager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taij\\documents\\calvin\\myprojects\\envcpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\tensor_array_ops.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    768\u001b[0m             \u001b[1;34m\"Could not read index %d twice because it was cleared after \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[1;34m\"a previous read (perhaps try setting clear_after_read = false?)\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 770\u001b[1;33m             index)\n\u001b[0m\u001b[0;32m    771\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_zero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Could not read index 1 twice because it was cleared after a previous read (perhaps try setting clear_after_read = false?)"
     ]
    }
   ],
   "source": [
    "array.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, variance = tf.nn.moments(array.stack(), axes=0)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-67df6bc22dc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvariance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'variance' is not defined"
     ]
    }
   ],
   "source": [
    "variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss  = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 200, 300,   4])>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.where([True, False, False, True], [1,2,3,4], [100,200,300,400]) # take second array's value when true, else take third array's value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAD8CAYAAACiqQeGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3gUVffA8e8JBEKvQbqoCMorBAxSBKWIioCgAr6IYBe7oGLBhr3gT0VARYovIohiQWkKKlWqoHSkCCJNQVoIEEg5vz/ugiEEsiG7md3N+TzPPtkyO3MmM7tnZ+bec0VVMcYYY0xoiPI6AGOMMcb8yxKzMcYYE0IsMRtjjDEhxBKzMcYYE0IsMRtjjDEhxBKzMcYYE0IsMRsTRkSkuYioiJTNpeXdIiKJubEsY4xjidmYIBORESIyMZPn6/uSbLXcj8oYE6osMRtjEJECXsdgjHEsMRsTIjI7TS0i1XzP1c8weSMRWSIiSSKyWETiM8zrYhGZKSIHRWSriLwvIsXTvT7D99z/ichOYE424rxLRNaLyBHf3zszeX2tL7adIjJFRPL7XqstIj+KSIKI7BeRpSLSIjv/J2MinSVmY8LT/wGPA/WBDcAkESkMLvkBU4HxQBxwHVAX+DDDPLoBAlwC3OTPQkXkWmAQ0B+4AHgHeE9Erva9Xh94F3geqAm0Ar5LN4tPgO1AA6Ae8ByQ5PdaG5MH5Pc6AGPyiNaZNKLKyQ/jF1V1CoCI3ApsAboCw4BHgc9U9c2jE4vIPcCvIlJOVXf4nt6oqo9kc7m9gY9VdZDv8Vrf0frjwASgKnAAGK+q+4FNwNJ07z8T+D9V/c33eH02l29MxLMjZmNyxyzcUWv6W9cczG/e0TuqmggsB2r5nooHuolI4tEb/56qPifdPBafxnLP58TT3j+lW/b3uGS8UURGi8jNIlIs3bRvAcNEZJqIPCUi551GDMZENEvMxuSOg6q6Pv0Nd5SbXprvr6R7Lvo0lhWFO3JO/yMgDjgXWJJuugOnMW+AzIakUwDfUfKFwPXAn0Af4DcRqeh7/TlcEv8auBhYJiK3nWYcxkQkS8zGhI6dvr8V0j1X9yTTNjp6R0SK4K73rvY99Qvwn4w/BHy3QzmMcTXQNMNzTYFVRx+oaoqqTlPVPkAdoAjQLt3r61R1gKq2BYYDd+QwJmMiil1jNiZ0rAc2A8+JyBNANeDpk0z7tK819TbgWeAIrmEVwOvAfBEZDHwA7AfOA65W1btyGOMbwOcishjXwKw1cCOugRki0g53unwWsBtoARQDVotIIVyjtc+BP4AzcEl9QQ5jMiaiWGI2JkSoarKIdAHewzWYWgI8CZxQnAR4AngT1/J5JdBOVQ/45rNMRC4FXgJmAvlwLbfHBSDGr0XkAVwjsP6468n3quoE3yR7gWtwPxYKA78Dd6jqbF9f6VLAR0B5YJdv3XrnNC5jIomoZna5yBhjjDFesGvMxhhjTAjxOzGLSD4R+fUkNX8LishnvipAC6z2rzHGGHN6snPE3JN/W31mdDuwR1WrA2/jGp8YY4wxJpv8SswiUhloi+sbmZkOuAYdAF8Al4mInGRaY4wxxpyEv0fM/YHH+LcAQkaVcN08UNUUYB9QJsfRGWOMMXlMlt2lfP0Sd6jqYhFpfrLJMnnuhObeItID6AEQExMTX7Vq1WyEGl7S0tKIijr17559+6KJiUmlYMGT/d4JXf6sXziL1PXbvHkzqkpe/+yFs3Bcv9RUISVF/PquC8f1y461a9f+o6qxp5rGn37MTYD2ItIGiAGKi8goVe2WbpotQBVgi294txK44gLHUdUhwBCAmjVr6po1a/xbkzA0Y8YMmjdvnuV0R46ACESfTuFFD/m7fuEqUtevefPm7N27lyVLlmQ9cZiK1G13VLit35YtkJgI5/lZFT3c1i+7RGRTVtNk+bNEVfuoamVVrQZ0AaZlSMrghpe72Xe/k28a6yDth5tvhh9/9DoKY4wJjmXLYNIkr6MIL6dd+UtEXgAWqep4XL3bj0VkPe5IuUuA4ot4//sfxMR4HYUxxgRHmzbuZvyXrcSsqjOAGb77z6Z7PgnoHMjA8oqYGBg6FBo0gLg4r6MxxpjA+d//YNs2eOopryMJL1YrOwRUrAiFC3sdhTHGBNZ//wu7T2htZLJiiTkEtG0Le/dCQgIUL+51NMYYk3NLl8Lhw+5soMmeyG2THmaeecYagRljIsf27bApy/bHJjN2xBwiBgxw3aaMMSbcJSdD69ZeRxG+7Ig5RIjA4MHwzTdeR2KMMTnz+uvw5pteRxG+7Ig5hFx8MZQq5XUUxhiTM336wKFDXkcRvuyIOYTUqQOqdl3GGBO+Jk6EuXOhaFGvIwlfdsQcYsaPh7Jl4cwzvY7EGGOyLybGiibllCXmEHP//V5HYIwxp+eff6BFC8iXz+tIwpudyg5BQ4fCG294HYUxxmRPv34wcqTXUYQ/O2IOQVdfDYUKeR2FMcZkT79+kBZ+o9iGHDtiDkHly8PWrTB/vteRGGOMf95+G376CSJ4KOVcY//CELV5M2zc6HUUxhjjn0aNoFo1r6OIDHYqO0RdeaX7m5wM0dHexmKMMaeyerXr7lmkiNeRRAY7Yg5hn3wCDz/sdRTGGHNqw4bBwoVeRxE5sjxiFpEYYBZQ0Df9F6raN8M0twBvAFt9Tw1S1WGBDTXvueYa6NTJ6yiMMebUrPxmYPlzxHwYaKmqcUBdoLWINMpkus9Uta7vZkk5AAoXhrVr4aOPvI7EGGMy160bLFnidRRhws8m61kmZnUSfQ+jfTc9/cicQlu2wL59OZ1NxCtc2LpOGWNC1zPPQK1aXkcRBtavh3r1/JpUVLPOsSKSD1gMVAfeVdXHM7x+C/AqsBNYCzykqpszmU8PoAdAPMTPrFaN5a++SlL58n4FG04SExMpGqBisarwzz8FiI09EpD5BUIg1y8URer69erVi9TUVAYOHOh1KEETqdvuqFBav59+KkvdunsoWjQ1YPMMpfULlBLLl3PB008TnZCAwGJVrX/KN6iq3zegJDAduCDD82WAgr77dwPTsppXvQIFVEG1XDnV+fM10kyfPj1g81q4UPXqqwM2u4AI5PqFokhdv2bNmmlcXJzXYQRVpG67o0Jl/dLSVHv1Ut21K7DzDZX1C5hRo1SP5rurrlJgkWaRH7PVKltV9wIzgNYZnt+lqod9D4cC8VnN62DVqnD55bBjBzRvDp9/np1Q8pT69W2cZmNMaElOdkVFSpf2OpIQpQrPPecuwh854gZCGD/er7dmmZhFJFZESvruFwJaAb9lmKZCuoftgdVZxhwVBZMmQY8ekJQE118Pr77qVsYcRwS2bIE77/Q6EmOMgZQUqF0bdu/2OpIQlZQEN94Izz/vSqENGAADB0J+/0qH+DNVBeAj33XmKGCsqk4UkRdwh+TjgQdFpD2QAuwGbvFr6dHRMHgw1KwJvXvDk0+6ZsgffAAFCvg1i7yiQgX320XVJWpjjPFK/vzw889QvLjXkYSgnTvh2mthzhw3KPWnn0LbttmahT+tspepaj1VraOqF6jqC77nn/UlZVS1j6r+R1XjVLWFqv526rmmI+KqaHz1lWuCPGIEXHGF/RTLIH9+d8Z/9myvIzHG5GWpqfDUU1CwoNeRhKDffnO1SefMgcqVXfHwbCZlCKXKX9dc47JOxYowc6ZbuXXrvI4qpCQlQf/+7jSSMcZ44fBhqFTJTmqe4McfXd7asAHi410ptLi405pV6CRmgAsvhAUL3MqsW+dWctYsr6MKGcWKuRMLNgi5McYru3bBvffaJbXjDB8OrVu72hzXXOMOLitUyPp9JxFaiRn+Pfxv186dzm7VykbeTufwYddHPTEx62mNMSaQtm93ZYJtzGWftDR4/HG44w53KvPRR+HLL3M8mkfoJWZwF8y//hp69XJt8m++2ZWXsb2BggVd16kI639vjAkDFSq4ceJtzGXg4EHo3Bn69XOnMYcMcfcD8M8J3X9vvnyuk9y777r7L70EXbu6C615XNWq7t9yJHQKgRljItzGje7A0E5h404dNGvmri2WKAHffRfQ/qyhm5iPuvdemDjRXWD97DNo2dIVJcnDRGDvXis1bozJPeXKwW23eR1FCFi2DBo2hEWL4KyzYN48d8k1gEI/MYO7qD5njjtUnDfP/VNWrfI6Kk8d7a5gZ/eNMcG2axcsXw4XX+x1JB6bPBmaNIHNm6FxY3de//zzA76Y8EjM4MrMLFgADRrAH3+4f8r333sdlac6doRff/U6CmNMpPv9d/j2W6+j8Ni778LVV7uWt126wLRp7jRCEIRPYgYoXx6mT3fNAhMS4Kqr3AX3PGrSJNddzhhjgiUtzR0PPf+815F4JDUVevZ0ta7T0uDZZ+GTTyAmJmiLDK/EDK462GefQZ8+7h92112unGdq4IYdCxcFCsA777iz/MYYEwxDhrhclCft3w8dOrha19HRruvu888HvQWcfxW1Q01UFLzyCpx7rhsE48033SDUo0fnuP9YuKlf3116N8aYYLjjjjza0HTzZnfqeulSN4TWuHFw6aW5sujwO2JO79ZbYepUKFXKde699FLYts3rqHJVkyauN9mmTV5HYoyJNJMmucEqypTxOpJctnixa2S8dCnUqOHaN+VSUoZwT8wALVq4ltrnnAO//OIuhixZ4nVUuWrcOKtcaowJvHz58mAJ4K+/dkl4+3Y3ctC8eVC9eq6GEP6JGdywkfPnwyWXwNat0LSp6/ucR9x3H3TvbkNZG2MCZ/t2N9BfgwZeR5JLVOH//g+uu85V9brlFpgyxZ3GzmWRkZgBypZ13ae6dYMDB9wF+3feyTPZatw4eOQRr6MwxkSKp55yeSlPSE6Gu+92ta5VXRumDz/0bAitLBt/iUgMMAso6Jv+C1Xtm2GagsBIIB7YBfxXVf8IeLRZKVjQtZqrUcM1I+zVC9audQk6f3i2c/NXixauQpwxxgTC8OFeR5BL9u51Na9/+MF1gRo50j32kD9HzIeBlqoaB9QFWotIowzT3A7sUdXqwNvA64ENMxtE3IAXn3ziEvV777mWdQkJnoWUG0qWdP3eP//c60iMMeGuRw9XVCTi62Jv3OjKmf3wgysWMmOG50kZ/EjM6hwdZDDad8t4frgD8JHv/hfAZSIeb9IbbnCVWWJjXYHxJk3yRNPlrVu9jsAYE+5uvBHOPNPrKILsaHnn1avhP/9xLa8bNvQ6KgBE/bgGKyL5gMVAdeBdVX08w+srgNaqusX3+Hegoar+k2G6HkAPgNjY2PixY8cGZCVOJWbbNmo/+SRFNm3iSKlSLH/pJfbXqhX05SYmJlLUo7EZExPzU7RoSpCX4d365YZIXb9evXqRmprKwIEDvQ4laCJ12x0V7PVbuLA09ertITram/Y5ubH9yk2bxnmvvUZUcjK769dnZd++pObSPtOiRYvFqlr/lBOpqt83oCQwHbggw/MrgcrpHv8OlDnVvGrUqKG5Zs8e1VatVEE1Jkb188+Dvsjp06cHfRmZ2bRJtW5d1bS04C7Hq/XLLZG6fs2aNdO4uDivwwiqSN12RwVz/Y4cUb3hBtUDB4K2iCwFdfulpam++KLLBaB6992qycnBW14mgEWaRa7NVqtsVd0LzABaZ3hpC1AFQETyAyWA3dmZd1CVLOlGBbnzTjeec+fO8OqrEdliu2pVWLgwD1wbMsYEnIhrnlO4sNeRBMHhw3Dzza4Nkgi8/bZrgxSCDYOzTMwiEisiJX33CwGtgN8yTDYeuNl3vxMwzffLIHRER8MHH8Abb7iN8uSTcPvtcOSI15EFXFqa6zWWlOR1JMaYcLF9uxsUJyKHkt21Cy6/HD7+2P3q+Ppr12snRI9g/DlirgBMF5FlwM/A96o6UUReEJH2vmmGA2VEZD3wMPBEcMLNIRE34MWXX0KhQvC//8GVV8Lu0Dm4D4SCBaFrV1dS3Bhj/FGhgmsvG3HfG2vXQqNGMHs2VKzo/rZvn/X7PORPq+xlqlpPVeuo6gWq+oLv+WdVdbzvfpKqdlbV6qraQFU3BDvwHLn2WrdxKlRwzeMbN3aDYESQNm1co8PDh72OxBgT6n7/3ZV7iLia2DNnuqS8fj3Uq+eu8114oddRZSnSfhv5Lz7eNY+Pi3O/qBo2dMk6gnz+Ofzxh9dRGGNCXXQ0VKnidRQBNmKEO329Z4+rZTFrFlSq5HVUfsm7iRncnjh7NrRt605nt2oFo0Z5HVXADBrkiqDlwaGqjTF++vtvN1ruddd5HUmApKW5eqK33upKbT70kKtZHEZd6PJ2YgYoVswNGdmzp2sI1r27K+cZYm3XTtdDD8Gnn3odhTEmVE2dCu+/73UUAXLokCsu9corblis996Dt94KuyGyQq+duBfy5YP+/eHcc+HBB+HFF901iQ8/dLVTw9gzz7jhqo0xJjPdu3sdQYD8/bcbvGjBAnfA9fnnrnFvGLIj5vTuu88NF1msGIwZAy1bws6dXkeVI2XKuLP1X3/tdSTGmFDTuzdMmuR1FAGwcqVrJ7RggSvmMHdu2CZlsMR8oquugjlz3MY9Wkt11Sqvo8qRokWhRAmvozDGhJqHH3ZjOIS1qVPdSmza5AaPXrAALrjA66hyxBJzZmrXdhv3oouOH30kTMXHwyWXuC4RxhgD8Nln7ipeWF/qGjzY9Q1NSHAVHWfMgPLlvY4qxywxn0z58m4jd+wI+/ZB69YwdKjXUZ22BQvghRe8jsIYEyrWrAnZwldZS011h/v33OPu9+njWrkWKuR1ZAFhiflUCheGsWPhiSfcxu/RAx59NCxr1jVpAh99lPV0xpjIt2eP63xSrpzXkZyGxETXt+vtt10H7A8/dK2wI6hkWeSsSbBERbkBL4YPd8XO/+//3FH0gQNeR5Ztf//t+ttbv2Zj8q7Dh10xrMREryM5DVu3wqWXwvjx7hz81Kmuv3KEscTsr9tucztByZKuiXOzZrBtm9dRZUu5cvDmm2HXpc8YE0AFC8KKFWFVb8P59VfXuOvXX+Gcc1zj3ObNvY4qKCwxZ0eLFjB/vtspFi92LbaXLvU6Kr+JuHZt779vNbSNyYs2bHCj30ZHex1JNk2Y4Fqwbtvm/s6fDzVreh1V0Fhizq6aNd1O0bQpbNniLt6GUUdAEXd9ae9eryMxxuS28uXdkMRhQ9UVf+rQwV0+7NYNvv8eypb1OrKgssR8OsqWdd2nunVzO0v79jBgQNiU8XzySdd48eBBryMxxuSW9eth9Wp3TBEWUlJc0aeHHnLfrS+8ACNHunPxEc4S8+kqWNDtJM8/71pp9+wJDzzgdqYw8Mgj8OOPXkdhjMktGzbAL794HYWfEhKgXTt33a1gQfjkE1dfOGz7d2VPlrWyRaQKMBIoD6QBQ1T1nQzTNAe+ATb6nvrq6LjNEU3E9TmoXt21DHz3XVfF47PPvI4sSx98EFG9C4wxp3DgAFxxhddR+KfgX3+5S4QrVrizk998EwHlybLHn6/mFOARVT0faATcJyK1MplutqrW9d0iPymn17UrTJvmdqLvvoMmTdzOFcKiolzj8tde8zoSY0yw9ewJX33ldRR+WLCA+HvvdUn5vPNcZaQ8lpTBjyNmVd0ObPfd3y8iq4FKQHgXkA60Jk3cTtS2LaxY4Xaus892zftDVIMGULeu11EYY4ItLIZ1/OIL6N6dAklJcNll7nHJkl5H5QnRbDRYEpFqwCzgAlVNSPd8c+BLYAuwDeitqiszeX8PoAdAbGxs/NixY3MQemjKn5jIf/r2pdQvv5BaoAC/PfkkO5s18zqsk9q3Lz/Ll5ekadN/svW+xMREioZdR0j/Rer69erVi9TUVAYOHOh1KEETqdvuqOysnyoMHFidrl3/pGzZI0GO7DSpUvWTTzh72DAA/rziCjY++iiaPzJHJW7RosViVa1/yolU1a8bUBRYDFyXyWvFgaK++22AdVnNr0aNGhqxjhzRrW3bqrrPheprr6mmpXkdVaa2bVN94onsv2/69OkBjyWUROr6NWvWTOPi4rwOI6giddsdlZ31S0tTHTdONTk5ePHkyOHDqrfe6r4nRVT79dPp06Z5HVVQAYs0i/zoV/MfEYnGHRGPVtUTrlSoaoKqJvruTwaiRSSyO5qdSnQ0ax95BPr1cw3EnngC7rgDjoTeL9YKFVzF0X37vI7EGBNIqq7Lb4cOrppwyNm9242Z/L//uf6bX37pxiLIIy2vTyXLxCwiAgwHVqvqWyeZprxvOkSkgW++uwIZaNgRcTvZl1+6ne7DD90IVXv2eB3ZCQ4fdteb9+/3OhJjTKDs3AmjRoVoeYX166Fx43+HaZw1C6691uuoQoY/R8xNgO5ASxFZ4ru1EZG7ReRu3zSdgBUishQYAHTxHbKba691O1358jB9utsZ16/3OqrjFCwIy5dDsWJeR2KMCYTUVChd2pVaCLlukbNnu3LGa9dCnTqwcCHUP/Ul17wmy02mqj+pqqhqHf23O9RkVR2sqoN90wxS1f+oapyqNlLVucEPPYzUr+92vjp13CCojRrBTz95HdVxChRwB/iLF3sdiTEmpyZNcjWxQ86oUdCqlTuN3aaN+x6sUsXrqEJOqP2WilxVqridsE0b2LXLdQcYNcrrqI7TuTOce67XURhjcurqq+Gdd7KeLteoQt++0L27a2vzwAOucIidpsuUJebcVKyY2xkffNDtnN27u501RM76N2jghjudP9/rSIwxp2vwYHfptnhxryPxSUqCG290ta6joty4AgMGhGiLtNBgiTm35c/vfsoOHOh20hdecDttUpLXkQGwaZOrqWuMCU/16kG1al5H4bNzpzs7OGaMGwB6wgR3tGxOyX6yeOX++11lsP/+1+20mza5GpmxsZ6G1bq1+7trF5Qp42koxphsmjbNVbCMifE6EtxQVm3bwsaN7lLexImunY3Jkh0xe6lNG5gzx+20c+e6loqrV3sdFevXu4FdQuQMuzHGD6owYkSIdHv88UfXA2XjRoiPd+WKLSn7zRKz1+rUcTvtRRe5nbhxY8/HY6xe3fVosH7+xoSPw4dd9yiPT7rBsGHu1Nu+fa676MyZrpKR8Zsl5lBQoYJrrdGxo9uZW7eGoUM9DSkqyn2mQnyQLGMM8McfbhwdT89ypaXB44+7flopKfDYY24giiJFPAwqPFliDhWFC8PYsW7HTkmBHj3cjp2W5kk4UVGukmjZvFtY1ZiwUa2a+23v2Vmugwddf8t+/VwD1yFD4PXXQ7C6SXiw/1ooiYpyAyQPG+Z27jfegE6d3CjnHmjY0J2FWrPGk8UbY/wwbhwMH+5hl+Dt26FZMzfgc4kSbkz6kKxuEj4sMYei22+HKVPcWKTjxrmdfts2T0LZts210DbGhKYLL/SwouWyZe4X/KJFcNZZMG+e6x5lcsQSc6hq2dLt5Gef7epkNmwIS5fmehjdu7sKonat2ZjQM3Wqu4QbF+fBwidPdhe2N292fbQWLIDzz/cgkMhjiTmUnXee29mbNIEtW6BpU1cEN5dNmQJPPZXrizXGZGH2bNi714MFDxrk6n4mJsINN7ieJJ43B48clphDXdmy8MMP0LWr+xC0b++qhuWiEGgkbozJYPduePFF170x16SmupLCDzzgGqY++yyMHh0iFU0ihyXmcBAT4wa8eO4592E4+sFIScmVxYu4iqFNm3rWDs0Yk87u3XDJJZCcnIsL3b8fOnRwBwYFCsDHH8Pzz1vBgyDIMjGLSBURmS4iq0VkpYj0zGQaEZEBIrJeRJaJyIXBCTcPE3EDXowa5T4Ugwa5o+eEhFxZfOHC7qjZuiQa473SpWHJEoiOzqUFbt7876W0MmXcWbxu3XJp4XmPP0fMKcAjqno+0Ai4T0RqZZjmKuBc360H8H5AozT/uvFGVxC3bFn49lv3Yfnzz1xZ9Pnnw0cfwW+/5crijDGZ+OmnsrzwQi4m5UWL3NBzy5ZBjRpu+LlLLsmlhedNWSZmVd2uqr/47u8HVgOVMkzWARipznygpIhYDbZgadLEfTjOOw+WL3cfmp9/zpVFFy2aK4sxxpzEhRfu4b//zaWFjRsHl17qumU0b+56iuTqRe28KVvXmEWkGlAPWJDhpUrA5nSPt3Bi8jaBdM45buCLli3h77//7eAfZB07QtWq8OefhYO+LGPM8UaPhr17o6lZM8gLUnUFjjp2hEOH4NZbXfeM0qWDvGAD2Rj2UUSKAl8CvVQ144XNzK7+n1C1VUR64E51Exsby4wZM/yPNMwkJibmyvpJnz7UiImhwuTJ0LEjv/foweYuXYLaIGPhwtIsXFiaqlVnBG0ZXsut7Zfb9u7dS2pqakSu21GRuu0AFi+uSO3awV0/SUnh3P79qejrmrnhzjv584Yb3IFALojk7ec3Vc3yBkQDU4CHT/L6B8AN6R6vASqcap41atTQSDZ9+vTcW1hammq/fqrud67q7berHj4c1EVOnz492IvwVK5uv1zUrFkzjYuL8zqMoIrUbbd8ufsb1PXbs0f1ssvc90hMjOrYscFb1klE6vY7ClikWeRcf1plCzAcWK2qb51ksvHATb7W2Y2Afaq6Pac/GoyfRODRR+HLL6FQIVc496qrYM+eoC3y8OEo4uJCZOxXYyLcP/+4XpJB7SG5YcO/w86ecYYbFaNz5yAu0JyMP9eYmwDdgZYissR3ayMid4vI3b5pJgMbgPXAUODe4IRrTum669yoE+XLu5bbjRvD778HZVEFC6Yxd66HhfONySNSU10PpWnT3Ng2QTF3rqu9+9tv8J//uIqDDRsGaWEmK1luZlX9icyvIaefRoH7AhWUyYGLLnIfqnbtXIvthg3h669dt6oAK1UK3n7bfY6vuCLgszfGACNGuN/Xr7wSpAWMGeMadx0+DFdeCZ995kaJMp6xyl+RqGpV+Okndzp71y432svo0UFZ1KWXQp06QZm1MQa45Rbo3TsIM1Z1NT27dnVJ+Z57YOJES8ohwBJzpCpeHMaPh/vvhyNHXJWe555zH8YAio93l7jHjw/obI0xwEMPwR9/BKGX0uHDcPPNrta1iDv19e67QTxXbrLDEnMky5/f1bUdMACiolxd227dXOHrADp0yBUFMvaJ3lUAACAASURBVMYE1hVXQOXKAZ7pP//A5Ze7WtdFisA330CvXlbzOoRYYs4LHnjAHdIWLQqffAKtWsHOnQGbfbVq8PTTrlFngA/IjcmTDhxwH9WrroKCBQM44zVrXCOv2bOhYkX39+qrA7gAEwiWmPOKtm3ddefKlWHOnH9bYAaIKtx+O2zaFLBZGpNn7dwZhA4VM2b821OjXj1YuND9NSHHEnNeEhfnPozx8e7wtlEj12cxAERcd45q1Vz3DmPM6dm2zfV4fOaZAM70f/9z58X37HGj0s2aBZWsanKossSc11So4Po6X3st7NsHrVvDsGEBmbWIm1XfvgGZnTF50pAh8OmnAZpZWho8+STcdpsbvPnhh11NfRuNJqRZE7y8qEgR+OIL6NMH+vWDO++Edevg1VddI7EcuP56a9hpzOlKTQ1g54lDh1zL688/h3z53Bjud9+d9fuM5+yIOa+KioLXX4ehQ10m7dfPld87eDBHsy1e3P0wP/oD3RjjnwMHoG5d9xHMcQPpv/+GFi1cUi5eHCZPtqQcRiwx53V33AHffeeKCnz1lRs+cnvOypwXL+7OlOfLF6AYjckDihRxTT4K53RE1RUrXMW/BQvgzDNdY08rzRdWLDEbVxls3jw4+2xYtMh9qJcuPe3ZibgeGD/84M6QG2NO7auv3LXlcuVyOKMpU6BJE9c94mhyvuCCgMRoco8lZuOcfz7Mnw8XXwybN7va2pMn52iWf/3lKoIaY06tQQPXSSJH3n/fdYtMSHCXpaZPd6NEmbBjidn8KzbWnUvr2hUSE91h76BBpz27m25yXzjLlwcwRmMizIcfutPYp11zPjXVtba+9153/8knXbPuQoUCGqfJPZaYzfFiYmDUKNfnKS3NVQ3LwUCwf/zh+mNaRTBjTqTqzjqfdnuMxEQ33Ovbb0N0tOuv/PLLOe5dYbxlW8+cSMT12Rg1CgoUcPW2O3SA/fuzPauzz3ajTh46ZMnZmPT273cVMp9/3jWYzLatW93wbuPHuzFYp051Q1GZsJdlYhaRD0Vkh4isOMnrzUVkn4gs8d2eDXyYxhM33uhObZcp4643N20Kf/55WrO66SZXEdAY4yxe7HornpZff3XXiX79FapXd+1DmjcPZHjGQ/6UghgBDAJGnmKa2araLiARmdDStKlr2dm2rRtCqmFDmDAh27P56CN3Hc0YA3v3ujx6Orm0zJw58MorrsPzJZe4Jt1lywY6ROOhLI+YVXUWsDsXYjGh6pxzXHeqFi1cU+tLL6XsrFnZmkWRIu7HfffuQYrRmDDSoQOsWpXNN6nC229zwTPPuKTcvTt8/70l5Qgk6seFPxGpBkxU1RM6xIlIc+BLYAuwDeitqitPMp8eQA+A2NjY+LFjx55u3CEvMTGRohFWj1aSk6nx9ttU+PZbAH7v0YPNXbr4XaYoJUXYurUQZ56Zs+piuSEStx9Ar169SE1NZeDAgV6HEjShvu1UITlZKFDA/0YXkppK9QEDqDR+PAAbb7uNTd26ReQYyqG+/XKqRYsWi1W1/iknUtUsb0A1YMVJXisOFPXdbwOs82eeNWrU0Eg2ffp0r0MIjrQ01ddeU3XfL6p33KF65Ei2ZvH886rr1gUpvgCJ1O3XrFkzjYuL8zqMoArlbTdrlupNN2XzTXv3ql5xhfu8FSyoK595JiixhYpQ3n6BACzSLPJjjocbUNWEdPcni8h7IlJWVf/J6bxNCBKBxx9nRVISF7z2mhtOasMGNyhGqVJ+zSI+3lUANSavufhiqFIlG2/44w9o1w5WrnR1Br75hh2HD1MrWAGakJDj7lIiUl7EnU8RkQa+eVq9pwj3T7NmbvjIM85wAzFffLHfI7u3beu6SH/1VZCDNCaEPPyw+4hUq+bnGxYscI0tV650lfkWLIDGjYMZogkR/nSXGgPMA2qKyBYRuV1E7haRo0OVdAJWiMhSYADQxXe4biJdgwawcCHUrg2//eZqCs6Z49dbk5Jg9eogx2dMCLnySqha1c+JP//cNdnesQNatYK5c+Gss4IZngkhWZ7KVtUbsnh9EK47lcmLqlaFn36CLl3g22+hZUtXfahr11O+7cwz4amn3FgZNWu6gmPGRKKNG2H2bNeXP0uq8NprrqwmuLHS333XVfUyeUbIDmmfkJDAjh07SA7TQX1LlCjB6gg+JMy4ftHvvEO5Sy6h+JNPusIk69bBs89m2Wp08GA3dvNFFwU7YmO8ceSInyU3jxyBu+6CESPc5+aNN9z57whseW1OLSQTc0JCAn///TeVKlWiUKFCSBjumPv376dYsWJehxE06ddPVTl06BBbO3WCSpUofuutrqTnunWucdgpDofff9/9/esvKF8+FwI3Jhf98IOr0VOzZhYT7t7tal7PnOkGZB49Gq65JldiNKEnJGtl79ixg0qVKlG4cOGwTMp5jYhQuHBhKlWqxI6LL3a1e4sWdV8urVrBzp2nfP+aNe5MuLVMMJFEFcaOhX+y6p+ybp1r1DVzJlSoALNmWVLO40IyMScnJ1PIhiwLO4UKFXKXHtq2ddedK1d2jcEaNXKNw06iZk1XkjslxbXWNibc7dvnzgINGeI+Bic1e7b7fKxdC3FxruV1fHyuxWlCU0gmZsCOlMPQcdssLs612I6Pd/2cGzd23apOIl8+6NULvvwyFwI1Jsh++MENynZKH38Ml13mTmO3beuSdLY6OZtIFbKJ2USAChXc6blrrnFV+6+8EoYPP+nkL78MnTrZKW0T3nbvho4d3f6cKVXXMPKmmyA52Y13/s03EMFtUkz2WGI2wVWkiDsMfvRRd676jjvgiScyPWddsiTs2eO6bx45kvuhGpNThw65IZITEk7SmDopyfVaePFFiIpyh9XvvONns22TV1hiNsEXFQX9+rkLbvnzw+uvw/XXuxFyMihdGj74AAoU8CBOY3IgJcV1QPjlFyhePJMJdu50/fzHjHGNIydOhPvvz/U4TeizxBxgzZs35/4cftgCMY+s7NmzhzPOOIPf/Sij2alTJ956662cL/TOO10RkhIl3FF08+awffsJk513niu9/e67OV+kMbnlhRdc78BMf1SuXu3Ka86b564jz5kDV12V6zGa8GCJOY965ZVXaNOmDeecc06W0/bt25eXXnqJffv25XzBrVq5L6ezzoKff3ZfVsuXnzDZRRe5S9LGhANVePxxuCGzOok//OAaP27c6HbsBQugTp1cj9GED0vMecgR34XbgwcPMmzYMG6//Xa/3le7dm3OPvtsRo0aFZhAzj8f5s93X1abN0OTJu5IOp0zz4RzznFfdln2AzXGQ+vWQZs2ri7ICcMIDx0KrVu7/lMdO8KMGa5RpDGnYIk5CNLS0nj++ecpW7Ys5cqVo3fv3qT5Gjtldpr6lltuoV27dsc9l5KSQs+ePSlVqhSlSpXi0UcfPTYPcNW2+vXrxznnnEOhQoWoXbv2CYmzefPm3HPPPfTu3ZvY2FiaNGkCwOTJk4mKijr2GKBfv36IyAm3Z599FoD27dszZsyYwP2TypVz3ae6dIH9+93QdhnOXYtAvXpQsGDgFmtMoFWvDgMGZGjslZYGjz0GPXpAaqr7hTl2rMvexmQhfBKziDe30zB69Gjy5cvH3LlzGTRoEP379+ezzz7L9jzS0tKYN28eH3zwAUOGDKF///7HXn/66acZPnw47777LqtWraJPnz7cddddTJo06bj5jBo1ClVl9uzZjBw5EoDZs2cTHx9/XL/je+65h+3btx+7PfLII5QvX56bfJX3GzRowMKFCzl06NBp/U8yFRMDn3ziuo6kpbmGMD17ui8yny5d3MHGBx8EbrHGBMott7jKdeeem+7JAwdcv7833nCNHYcNcwNTRIXP163xVkjWyg53tWrV4umnn6ZYsWLUqFGDoUOH8uOPP3JDphegMlehQgUGDBiAiHDeeeexdu1a3nrrLR5++GEOHDjAW2+9xdSpU7nkkksAOOuss1i4cCHvvvsubdu2PTafs846izfffPO4eW/atIkKGU6nFStW7Fjt69dff50xY8YwY8YMqlevDkDFihVJTk5m27Ztfl2X9psIPP+8+2a7/XZ36PH7767lqi+eggVdi1djQs2997oj5mO2bYP27WHxYtf/78svXUtsY7IhfH7CqXpzOw11MjTsqFixIjt27MjWPBo1anTcEW3jxo3ZunUrCQkJrFq1iqSkJFq3bk3RokWP3d5///0TWlnHZ1Le79ChQ8ScZGCJV199lQEDBjB9+nRqpqu8f7REakCPmNPr1s01kilTBiZNcpX/N28GIDYW7rvPXZ5buTI4izcmOyZOdAfCDRq4g2LAjWHasKFLymef7Ro5WlI2pyHLI2YR+RBoB+xQ1QsyeV2Ad4A2wEHgFlX9JdCBhpPoDGOnisix68NRUVFohoSf3aEtj85rwoQJVM0w8nrGZRcpUuSE95ctW5Y9e/ac8PzLL7/M4MGDmTlz5rEj5aN2794NQGxsbLZizZZLLnGNwtq2hWXL3LfehAlQvz7gag9bpVYTCmrXhooV0z0xaZK77pKY6BozjhvnflEacxr8OWIeAbQ+xetXAef6bj2A93MeVuSKjY1le4a+u0uXLj1hugULFhyXwOfPn0/FihUpXrw4tWrVomDBgmzatInq1asfdzvzzDOzjKFevXqsWrXquOdefPFFPvjgg+NOX6e3YsUKKlasyBlnnOHvqp6e6tXdkUaLFi4TX3qp+5LDfe9deqk7O5juMrQxuWbXLjdEcpUqcOGFvicHDnSnrxMToWtXd+bHkrLJgSwTs6rOAnafYpIOwEh15gMlRcT6A5xEy5Yt+fbbbxk/fjxr1qzh4YcfZrPvlG1627Zto1evXqxZs4YvvviCN954g4ceeghw14N79+5N7969+fDDD1m/fj1Llixh8ODBDBkyJMsYrrzySlavXs2uXbsAd6T8zjvv8Omnn1KkSBH++usv/vrrL5KSko69Z/bs2bRufarfZwFUujR89x3cequrcdixo2tIo0pqqhuJau/e3AnFmPQKF3Znq6OicA0fHnjA1bpOS3NjkI8adcrxx43xRyAaf1UC0meWLb7nTijpJCI9cEfVxMbGMmPGjExnWKJECfbv3x+A0HJfamoqR44cITU19dg6JCcnk5KSwv79++ncuTOLFi3i1ltvBeCOO+6gXbt27Nq169j0qampXH/99Rw6dIiGDRsiInTv3p077rjj2DSPPfYYJUqUoF+/ftxzzz0UK1aMOnXq0LNnz+Pmc+TIkRP+l9WqVSM+Pp4RI0Zw55130q9fPxISEo7rPgUwfvx4mjdvTlJSEuPGjeOrr746bt6ZbaOkpKSTbtds696dqvnzc/bQofDYY2ybOZN1vXpx/fX5+fnnKFauLE58fHAydGJiYuDWI4Ts3buX1NTUiFy3o4K17caMqcJll+3gjDMOM3vyQWq98AJlFiwgLTqa3x59lB3NmrlBW4IsUvfNoyJ9/fyiqlnegGrAipO8Nglomu7xj0B8VvOsUaOGnsyqVatO+lq4SEhI8DqEU/r222+1Ro0ampKSkuW0gwYN0ssvv/y45062fkHZdp9/rhoT45rjXXaZ6p49um6d6gMPBH5RR02fPj14M/dQs2bNNC4uzuswgioY2y4tTfV//1Pdt09VN21SrV3b7Y9lyqjOnh3w5Z1KpO6bR0X6+gGLNIv8GIhW2VuA9IOIVga2BWC+Johat27Nfffdx5YtW7KcNjo6moFZDi4bRJ06uSORM85w57EbN6Z61AYGDHBVDtet8y40E/mmTHG73S23QPG1i/4tI1uzpiuv2bSp1yGaCBOIxDweuEmcRsA+VT1xZAITch588EG/Gov16NHjuK5TnmjQwH0JXnAB/Pab+3KcO5dZs1xDbmOCpXBhX8Gur75yrQ//+ss1Tpw3z9WNNSbAskzMIjIGmAfUFJEtInK7iNwtInf7JpkMbADWA0OBe4MWrcnbzjzTjcrTurUroN2yJTcXGEP37vDTT26oW2MCZfNmN1TyJU2Vi3/q5xohHjoEt93mGieWKuV1iCZCZdn4S1VPWa7Kd878voBFZMypFC/u+jb37Anvvee6p6xbx6d/P0Pp0kKtWl4HaCJF/vxQpkQK9LjHVRMBV1rzscesQ70JqvCp/GXMUfnzw6BB0L+/+4Ls25dBCTdx/tmHmTz5tAu2GQNAcrIbW7mk7qHbqNYuKcfEuEHCH3/ckrIJOkvMJjyJuKPmb76BIkVg1CgOtmzHZx8lcfCg18GZcFcq9R8KtGzqWn2dcYZrfNixo9dhmTzCErMJb1df7S4wV6pEkXk/8NHiC9A1a5kyxevATDh68UXYOn4xD7x3PvnWrHK1NxcscI0PjckllphN+KtbFxYudDUSf/+d7c1vYNqITV5HZcJQzX9+olTXq1zjwtat3Y8+P3ouGBNIlphNZKhYEWbNgg4dOHf/L7z+RXV+e+Urpk/3OjATDt5/T5l60yiuH3AJJY7sdOM5TpjgGhsak8ssMZvIUaSIG+Gid29ISeGfp95iy1tjXR1jY07m8GHqT+jLeR8/6Ypgv/OOa1yY34arN96wxGwiS758bsCLDz6gab75dJ/4X75q8iY/z7ZOzuZE40cl0O+8D7nouxepWmS3a0z44IPW8tp4yhJzgLVv355SpUrRvXt3r0PJ23r0gG+/hRIlKDh/BtH33O4qNhnjo7+tofEzrbj6jwFQqZK7ntyunddhGWOJOdAeeughRo4cme33bd68mebNm1OrVi3i4uL46quvghBdHnP55TB3Lm2rraLuyk944bzRrPp6rddRmRAwb9BiutdZSuwfP3P+hYVd48G6db0OyxjAEnPAtWjRgmLFimX7ffnz56d///6sWrWK77//np49e3LQOuTmXK1arrtL48ZctO8HKnRv5copmjzr8JCPaNDrYl5Mfhw6dHCNBitW9DosY46xxBwiKlSoQF3fL/Zy5cpRqlQp/vnnH4+jihDlysG0aVzVpSQlEzfT6aoDbHj+Y6+jMrktLY1N977OJXedj6Qmc1bvTq6xYJEiXkdmzHGs2WEIWrRoEcnJyVSpUiXriY1/YmJg9Gjk3HN56sWXqPbcUlJ2/Ur+t99wDcZMZDt0iD87PcyZkwfzXVRZot4f7NohGBOC7Ig5xOzatYubbrqJ4cOHI9YyNLCiouCFF6g38mFS8hfiooHd2X3VjbB/v9eRmWD66y92NWlP58m3cKRYGUp/94klZRPSLDHnon79+iEiJ9yeffZZAA4fPsy1115Lnz59uPjiiz2ONoJ1706BH7/lu5I3UPr7z9jcsJMb489EHF2+gil1HqX0rz8w78wbKDB/lmsUaEwIs8QcYK1ataJz585MnTqVypUrM2/evGOv3XPPPWzfvv3Y7ZFHHqF8+fLcdNNNqCq33HILLVu2tK5WueHSSznj54kcrF6Ha1e/zP6LWsLixV5HZQJpyhQOXtyKkTtbc+CiFkQtnI+NC2rCgV+JWURai8gaEVkvIk9k8votIrJTRJb4bncEPtTw8MMPP7Bz507+/vtvtmzZQuPGjY+9VqxYMcqXL0/58uX56KOPGDNmDDNmzKB69erMmTOHzz77jK+//pq6detSt25dli9f7uGa5AHVq1N4wXQWNnuMon+vZ/zFr6HjvvY6KhMAhwd8QN+rFhKVuI/R/51A0ZmTXCNAY8JAlo2/RCQf8C5wObAF+FlExqvqqgyTfqaq9wchxojz6quvMmjQIKZPn06NGjUAaNq0KWlWOjL3lS5N1NTv2H9bT8aPjufy67pS6I0XID7e68jM6UhN5exB7xL95ZeU5y6inngcXn7WtS8wJkz4s7c2ANar6gZVPQJ8CnQIbliZe+45dwOoUQPWrnVnH49+hz7yCLz5prtfsSJs2wYzZkDz5u65Hj1gyBB3v1gx1+ZnwgQ3ciBA167wySfu/um0u0p/3bh48eInXEsGePnll3nvvfeYOXPmsaRsPFagAMU+fo9hr+xEgQceLUiVfgMhOdnryEx2JCaS1P56bvryPhLyl+GejxpT8NXnLCmbsCOqeuoJRDoBrVX1Dt/j7kDD9EfHInIL8CqwE1gLPKSqJ7SmEZEeQA+A2NjY+LFjx2a6zBIlSlC9evXTWR9PbdmyhR49erBz507y589Pnz59aN++/bHXX3/9dT766CMmTpzI2Wef7WGkOZeamkq+TLoZrV+/nn379nkQUWCUnjaTDS8voUPa1+y8sAFrn3+GlKJFvQ4rYHr16kVqaioDBw70OpSAitm2jXJ9+nP2nz+zMqYOR167jX1xcV6HFRSJiYkUjaB9MqNIX78WLVosVtX6p5xIVU95AzoDw9I97g4MzDBNGaCg7/7dwLSs5lujRg09mVWrVp30tVC2bds2/fXXX1VV9ffff9fKlSvrgQMHVFX1pZde0jJlyuicOXN0+/btx26HDh3yMuTTlpCQkOnz4brtjjN/vu4pWVHrsEQTasSr/v671xEFTLNmzTQuLs7rMAJr6lQ9ULKixvGrJpx7oc7/+GOvIwqq6dOnex1CUEX6+gGLNIv86M85ni1A+koXlYFtGZL7LlU97Hs4FMiTF+jSV++KjY09Vr1LVenXrx+7du2iSZMmVKhQ4dhtzpw5HkdtTvDHH+y8qRNTaz5IsbWLWRR3OzpxktdRmYxUOfx6f4ZfOZZCe7exqE1fivW5n6JrrR66CW/+JOafgXNF5CwRKQB0Acann0BEKqR72B5YHbgQw9Mvv/xyrHqXiLBv375MfxlddtllXodqMnr/fWJnzuSMBeNJbnctfRN7s+vqm+GppyA11evoDEBiInTrRuoTT7JOz+FIn+fIP2EcfPQRlcaPz/r9xoSwLBOzqqYA9wNTcAl3rKquFJEXROToBdQHRWSliCwFHgRuCVbA4WDXrl3cddddVr0r3JUoQfQ3XzDptRWUkT08+kpJ/ry0G+zY4XVkeduSJWyu05Y2n9xITOF8vPZlDQq+0tcaeZmI4VetbFWdDEzO8Nyz6e73AfoENrTwdLR618MPP2zVuyJBVBQ8/jjSoAGNrxnJGXO/Yl/cr5T4eBC0auV1dHmLKskDB7Ou9wecn7yU16uXIeqbBVY0xEQc+4kZQJquetcNN9zgdTgmkFq04LrVL1OwaQPu+OtFZlz+Ejz8MCQleR1Z3rB7N3TsyJKeH/Jm8gPIXXdRe9loS8omIlliDqD01buaNGli1bsiTcWKMH06Hz+zjmZRP/Ht26uZU+tOWLbM68gi24QJfH/uvQwcV4mLiq9l+NjiMHgwFCrkdWTGBIUN+xhA6at37d+/n2LFinkckTktX3zByjlzaJLZa/nzE/PCk9CuFfmufZ98G1eTUr8RKc88T8wTvSA6OrejjVx79rD9zmcp9OXH1KAEZeNawrglcNZZJ3/PqbadMWHCjpiNyahsWZJLlDj1NA0acMXaQTS6qy7jk1vzwLMloX59WLgwd2KMdBMmwH/+Q/8vKzMj+grOfKsX9RYPO3VSBv+2nTEhzhKzMRmNGEH5777LeroiRWDwYK6beg/vnPk2e5dtonfD2aT1fMjGeD5df/yBXnMtN7ffze/bC/Fa4/Fcs+IleOghyKTS3An83XbGhDBLzMZklN0v98svp/CqReTr+QDx8gtRA/qz6twO6KjRYAOT+CcpiUN9X+ObGo8i33zNPYU+okq/B5HZs1xhfH9ZYjYRwBKzMYFQuDDF+r/IDb88isbXp9ffT7Cl+xOkNW4Cc+d6HV3oUkXHfc3BWvU58sKrTE5uRep/u9Jo3ccUeLSnf0fJxkQYS8zGBFLdusiC+Uz9cCtVyqfw9MKrGdFkCHTpAr//7nV0oWXGDGjcmLHXjeHhjfdTolZlPphWg3yfjoZKlbyOzhjPhGxi1ixGvTKhx7aZT758cOutsG4dTz2WwrUFv2XjZwt49twx7vn1672O0FuLF0Pr1jzQYjlzFuTjutifGNQ/FZYsgRYtvI7OGM+FZGKOjo7m0KFDXodhsunQoUNEW3ehfxUtSpHXn6XEukUU7XI1jWU+jBjByBovsbvLvW5A8bxCFX78kb0trqVv/YnolCncXWQU8X2vJnrDGvL3vM+6mhnjE5KJuVy5cmzdupWDBw/aUVgYUFUOHjzI1q1bKVeunNfh5NzkySx77bXAza9KFWLHDOCqte+gt97GWmqQ/7NRrKvZjt+a3w1Tp7rEFYlSUmDsWBbVuomfWvWl2IzxxBZIIPWRx/nPpsnEPPcEBHLs3UBvO2M8EJIFRooXLw7Atm3bSE5O9jia05OUlERMTIzXYQRNxvWLjo7mjDPOOLbtwlrhwqQFY9udcw7y4XBeemYjvLKR70fsZ+/MZM6beSWzqtxI00cbE3VTN4iEfrgbNqDDhvPJ+3vpsncwe2hJUomzyPdYG+6/5x4oVSo4yw3WtjMmF4VkYgaXnMP5S37GjBnUq1fP6zCCJqLX7733qLh2LTRvHpz5n3UWDB1Kx1d2wtCh7B9Ug7c2d+LiBzux8pEPKX55Q868qzW0bg0FCgQnhmDYvx8mTmT9oO/4c+5mWjKdxbzJ5ec05PLeHeHmm4NfRjPY286YXBCSp7KN8dTYsZSbMSP4y4mNhSefpNimFXw9Npn8LS5lfnI8v0zeTlqHaxhW6lHS7rwLxo+HAweCH8/p2LMHRo6E9u15vdRrJHa9k11zf2ND/prQvTtvzbqIcuvmwN13505t69zadsYEkV9HzCLSGngHyAcMU9XXMrxeEBgJxAO7gP+q6h+BDdWYCBUdDZ07Q+fO3Ll5M4wZw54RO1i7uhJRwx7n02H7KJ5/OG0uO4xe1QZp3gwuuMCbPr5JSTB/Pke+n0mBWT8wbW4MS9Jq8zATKEElDjVsQcNuV9LwxhuDd7ramAiXZWIWkXzAu8DlwBbgZxEZr6qr0k12O7BHVauLSBfgdeC/wQjYmIhWAck34gAABvxJREFUpQo89hilHnuMfsuWwTeHOXfsegquWMnhKWupPWUAy6nN9sLV2X9BY2pfXt7V6D7vPDjnnMC2bD5yBFatgl9/hSVLWD1nN2nLV1LryK9UZxNL6c+5UpzSF50Ft77H3ddcAxUqBG75xuRR/hwxNwDWq+oGABH5FOgApE/MHYDnfPe/AAaJiKg1qTbm9NWpA3XqEP8MsGMHTJnC9HEDKbikEqs3VmXDwoLUXvgyNzOCp7masvn28tMZHbk6fhsHY8+kQIUy5C9fFsqVg+LFIX9+2LuX/AcOwLx57vT4/v3Hbslbd3Bg4w5Kbl/Nt6urccGumRRIPcj1jGUmt7GM6xGq8586qay9dCAxV4yk1CWXUKVkSa//U8ZEFMkqd4pIJ6C1qt7he9wdaKiq96ebZoVvmi2+x7/7pvnnZPMtXLiwNmjQIACrEJr27t1LyQj+woro9VuyhJSUFPLXr+91JCd35AgkJEBCAon7Uil8eA9HDit/UZ5q/MEGzqIgh6nENn6lLrVZziEKs4JkCpFEGSpTlAOUYg8LacBFLOQfyrKH0tRgLVuoRCn2UqRQGgdjSlO4ZAHXralYsdDubxwO2y6HIvqzR+Sv38yZMxer6il3UH8Sc2fgygyJuYGqPpBumpW+adIn5gaquivDvHoAPXwPLwBWZG+VwkpZ4KQ/TCKArV/4iuR1A1u/cBfp61dTVYudagJ/TmVvAaqke1wZ2HaSabaISH6gBLA744xUdQgwBEBEFmX1qyGc2fqFt0hev0heN7D1C3d5Yf2ymsaf7lI/A+eKyFkiUgDoAozPMM144Gbf/U7ANLu+bIwxxmRflkfMqpoiIvcDU3DdpT5U1ZUi8gKwSFXHA8OBj0VkPe5IuUswgzbGGGMilV/9mFV1MjA5w3PPprufBHTO5rKHZHP6cGPrF94ief0ied3A1i/c5fn1y7LxlzHGGGNyj5XkNMYYY0JISCRmEektIioiZb2OJZBE5EURWSYiS0RkqohU9DqmQBKRN0TkN986jhORiOl8KCKdRWSliKSJSMS0EBWR1iKyRkTWi8gTXscTSCLyoYjs8NVViDgiUkVEpovIat++2dPrmAJFRGJEZKGILPWt2/NexxQMIpJPRH4VkYmnms7zxCwiVXDlPv/0OpYgeENV66hqXWAi8GxWbwgz3wMXqGodYC3Qx+N4AmkFcB0wy+tAAiVded2rgFrADSJSy9uoAmoE0NrrIIIoBXhEVc8HGgH3RdD2Owy0VNU4oC7QWkQaeRxTMPQEVmc1keeJGXgbeAyIuIvdqpqQ7mERImwdVXWqqqb4Hs7H9XGPCKq6WlXXeB1HgB0rr6uqR4Cj5XUjgqrOIpP6CZFCVber6i+++/txX/CVvI0qMNRJ9D2M9t0i6vtSRCoDbYFhWU3raWIWkfbAVlVd6mUcwSQiL4vIZuBGIu+IOb3bgG+9DsKcUiVgc7rHW4iQL/a8RkSqAfWABd5GEji+07xLgB3A96oaMevm0x93EJqW1YR+dZfKCRH5ASifyUtPAU8CVwQ7hmA61fqp6jeq+hTwlIj0Ae4H+uZqgDmU1fr5pnkKd5ptdG7GllP+rFuEkUyei6ijkrxARIoCXwK9MpyVC2uqmgrU9bVVGSciF6hqRLQXEJF2wA5VXSwizbOaPuiJWVVbZfa8iNQGzgKWigi406C/iEgDVf0r2HEFysnWLxOfAJMIs8Sc1fqJyM1AO+CycKv2lo1tFyn8Ka9rQpiIROOS8mhV/crreIJBVfeKyAxce4GISMxAE6C9iLQBYoDiIjJKVbtlNrFnp7JVdbmqllPVaqpaDfelcWE4JeWsiMi56R62B37zKpZgEJHWwONAe1U96HU8Jkv+lNc1IUrcEcxwYLWqvuV1PIEkIrFHe3WISCGgFRH0famqfVS1si/XdcGVrc40KUNoNP6KZK+JyAoRWYY7ZR8x3Rt8BgHFgO99XcIGex1QoIjItSKyBWgMTBKRKV7HlFO+hnpHy+uuBsaq6kpvowocERkDzANqisgWEfn/9u4YpWIgCsPof5fhWqy0EwQLsRDRxkq3Y+1WdAE2bkQ74YHFWDiF4sNKyeXlnDLVbZKPZMjM9dIz/bH9JJdJDuf99jzfwHbBXpLH+ax8yuca86+/FO0yO38BQCPemAGgEWEGgEaEGQAaEWYAaESYAaARYQaARoQZABoRZliRqnr4sjnFpqrOlp4J+M4GI7BCVXWT5CDJ+Tw8AGji3w+xAHqpqqskR0lORRn6EWZYkfnp+iLJyRjjfel5gJ+EGVZingl7m+R4jLFZeh5gO2vMsBJV9ZLkNcnbvHQ3xrhfcCRgC2EGgEb8LgUAjQgzADQizADQiDADQCPCDACNCDMANCLMANCIMANAIx8ufoDAVNU/tgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "z = np.linspace(-4, 4, 200)\n",
    "plt.plot(z, huber_fn(0, z), \"r-\", linewidth=2, label=\"huber($z$)\")\n",
    "plt.plot(z, z**2 / 2, \"b:\", linewidth=1, label=r\"$\\frac{1}{2}z^2$\")\n",
    "plt.plot([-1, -1], [0, huber_fn(0., -1.)], \"r--\")\n",
    "plt.plot([1, 1], [0, huber_fn(0., 1.)], \"r--\")\n",
    "plt.gca().axhline(y=0, color='k') # Makes axis darker\n",
    "plt.gca().axvline(x=0, color='k')\n",
    "plt.axis([-4, 4, 0, 4])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Huber loss\", fontsize=14)\n",
    "plt.show()\n",
    "# Note how when Huber is in abs(1) it starts curving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:] # index 0 is the number of samples\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.5441 - mae: 0.8968 - val_loss: 0.2384 - val_mae: 0.5272\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.2084 - mae: 0.4982 - val_loss: 0.1875 - val_mae: 0.4741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2260c013fc8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/my_model_with_a_custom_loss.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"models/my_model_with_a_custom_loss.h5\",\n",
    "                                custom_objects={\"huber_fn\": huber_fn}) # We need this for custom objects to map the function name to the actual function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.1979 - mae: 0.4857 - val_loss: 0.1838 - val_mae: 0.4613\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.1942 - mae: 0.4801 - val_loss: 0.1993 - val_mae: 0.4816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2260be30248>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid)) # notice how it picks up where it left off and continues training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn1(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.2154 - mae: 0.4808 - val_loss: 0.2460 - val_mae: 0.4816\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.2125 - mae: 0.4787 - val_loss: 0.2263 - val_mae: 0.4762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2260bf26048>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/my_model_with_a_custom_loss_threshold_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"models/my_model_with_a_custom_loss_threshold_2.h5\",\n",
    "                                custom_objects={\"huber_fn1\": create_huber(2.0)}) # huber_fn1 is what we returned in the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.2103 - mae: 0.4753 - val_loss: 0.1933 - val_mae: 0.4538\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.2073 - mae: 0.4710 - val_loss: 0.2504 - val_mae: 0.4928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2260e604988>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold} # ** unzips a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.7297 - mae: 0.9225 - val_loss: 0.5269 - val_mae: 0.6453\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.2439 - mae: 0.5107 - val_loss: 0.4294 - val_mae: 0.5866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2260f72e108>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/my_model_with_a_custom_loss_class.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown loss function: HuberLoss",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-267cc011d931>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model = keras.models.load_model(\"models/my_model_with_a_custom_loss_class.h5\", # note how this time we need not provide the threshold 2 to Huberloss\n\u001b[1;32m----> 2\u001b[1;33m                                 custom_objects={\"HuberLoss\": HuberLoss})\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Future tf versions may fix this issue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taij\\documents\\calvin\\myprojects\\envcpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    144\u001b[0m   if (h5py is not None and (\n\u001b[0;32m    145\u001b[0m       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taij\\documents\\calvin\\myprojects\\envcpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Compile model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m       model.compile(**saving_utils.compile_args_from_training_config(\n\u001b[1;32m--> 184\u001b[1;33m           training_config, custom_objects))\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m       \u001b[1;31m# Set optimizer weights.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taij\\documents\\calvin\\myprojects\\envcpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saving_utils.py\u001b[0m in \u001b[0;36mcompile_args_from_training_config\u001b[1;34m(training_config, custom_objects)\u001b[0m\n\u001b[0;32m    232\u001b[0m   \u001b[0mloss_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Deserialize loss class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'class_name'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloss_config\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[0mloss_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m   loss = nest.map_structure(\n\u001b[0;32m    236\u001b[0m       lambda obj: custom_objects.get(obj, obj), loss_config)\n",
      "\u001b[1;32mc:\\users\\taij\\documents\\calvin\\myprojects\\envcpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taij\\documents\\calvin\\myprojects\\envcpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(name, custom_objects)\u001b[0m\n\u001b[0;32m   1173\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m       printable_module_name='loss function')\n\u001b[0m\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taij\\documents\\calvin\\myprojects\\envcpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     (cls, cls_config) = class_and_config_for_serialized_keras_object(\n\u001b[1;32m--> 292\u001b[1;33m         config, module_objects, custom_objects, printable_module_name)\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taij\\documents\\calvin\\myprojects\\envcpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[1;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprintable_module_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m   \u001b[0mcls_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown loss function: HuberLoss"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"models/my_model_with_a_custom_loss_class.h5\", # note how this time we need not provide the threshold 2 to Huberloss\n",
    "                                custom_objects={\"HuberLoss\": HuberLoss})\n",
    "# Future tf versions may fix this issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.2322 - mae: 0.4985 - val_loss: 0.3470 - val_mae: 0.5358\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.2248 - mae: 0.4909 - val_loss: 0.2942 - val_mae: 0.5096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22610949148>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1])) # variance = 2.0 / (fan-in +fan-out)\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights)) # 0.01 is lambda\n",
    "\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(1, activation=my_softplus,\n",
    "                           kernel_initializer=my_glorot_initializer,\n",
    "                           kernel_regularizer=my_l1_regularizer,\n",
    "                           kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=my_l1_regularizer,\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 1.6051 - mae: 0.8787 - val_loss: inf - val_mae: inf\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.6192 - mae: 0.5241 - val_loss: inf - val_mae: inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2261098e488>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/my_model_with_many_custom_parts.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"models/my_model_with_many_custom_parts.h5\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": my_l1_regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })\n",
    "# Tensorflow 2.1.1 works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(keras.regularizers.Regularizer): # We can do this if we want to be able to customize lambda\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=MyL1Regularizer(0.01),\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 1.8752 - mae: 0.9863 - val_loss: 2.2402 - val_mae: 0.5707\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.5767 - mae: 0.5237 - val_loss: 1.5281 - val_mae: 0.5152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2260d31d748>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/my_model_with_many_custom_parts.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"models/my_model_with_many_custom_parts.h5\",\n",
    "    custom_objects={\n",
    "       \"MyL1Regularizer\": MyL1Regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })\n",
    "# Tensorflow 2.1.1 works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 1.7564 - huber_fn1: 0.7326\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.5586 - huber_fn1: 0.2421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22613195948>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 11610 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.1158 - huber_fn1: 0.2305\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.1113 - huber_fn1: 0.2228\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_train))\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610,)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.115807869520863, 0.1112947969832983],\n",
       " 'huber_fn1': [0.23049977, 0.22281739]}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.115807869520863, 0.11565849124711532)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"loss\"][0], history.history[\"huber_fn1\"][0] * sample_weight.mean() # loss = metric * mean of sample weights (plus some floating point precision error)\n",
    "\n",
    "# metric is the sum of the weighted instance losses divided by the sum of all weights, i.e., the weighted mean of all the instance losses\n",
    "# loss is the sum of the weighted instance losses divided by the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.115807869520863, 0.1112947969832983],\n",
       " 'huber_fn1': [0.23049977, 0.22281739]}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1]) # True/Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0]) # Note it's not 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states() # reset both variables to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom streaming class, it is gradually updated, batch after batch\n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold) # TODO: investigate why this fails\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "#     def huber_fn(self, y_true, y_pred): # workaround\n",
    "#         error = y_true - y_pred\n",
    "#         is_small_error = tf.abs(error) < self.threshold\n",
    "#         squared_loss = tf.square(error) / 2\n",
    "#         linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "#         return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None): #  called when you use an instance of this class as a function \n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self): # the update_state() method gets called first, then the result() method is called, and its output is returned\n",
    "        return self.total / self.count\n",
    "    def get_config(self):  # get_config() method to ensure the threshold gets saved along with the model\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference\n",
    "# def create_huber(threshold=1.0):\n",
    "#     def huber_fn1(y_true, y_pred):\n",
    "#         error = y_true - y_pred\n",
    "#         is_small_error = tf.abs(error) < threshold\n",
    "#         squared_loss = tf.square(error) / 2\n",
    "#         linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n",
    "#         return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "#     return huber_fn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=14.0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = HuberMetric(2.)\n",
    "\n",
    "# total = 2 * |10 - 2| - 2²/2 = 14\n",
    "# count = 1\n",
    "# result = 14 / 1 = 14\n",
    "m(tf.constant([[2.]]), tf.constant([[10.]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total = total + (|1 - 0|² / 2) + (2 * |9.25 - 5| - 2² / 2) = 14 + 7 = 21\n",
    "# count = count + 2 = 3\n",
    "# result = total / count = 21 / 3 = 7\n",
    "m(tf.constant([[0.], [5.]]), tf.constant([[1.], [9.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'total:0' shape=() dtype=float32, numpy=21.0>,\n",
       " <tf.Variable 'count:0' shape=() dtype=float32, numpy=3.0>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'total:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'count:0' shape=() dtype=float32, numpy=0.0>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.reset_states() # we did not implement this so it used the default ==> reset to 0\n",
    "m.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[HuberMetric(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.8646 - huber_metric: 0.8646\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.2564 - huber_metric: 0.2564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2261444a108>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/my_model_with_a_custom_metric.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.2349 - huber_metric: 0.2349\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.2286 - huber_metric: 0.2286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x226145c25c8>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics[-1].threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787945, 1.        , 2.7182817 ], dtype=float32)>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponential_layer([-1., 0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 66233.5680 - val_loss: 18662058550442885943476092928.0000\n",
      "Epoch 2/5\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 32.4438 - val_loss: 17632522328030297767505035264.0000\n",
      "Epoch 3/5\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 32.0129 - val_loss: 23047029300062756774573244416.0000\n",
      "Epoch 4/5\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 31.5073 - val_loss: 224677720250185089960470642688.0000\n",
      "Epoch 5/5\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 30.8829 - val_loss: 13163095458361068364115214336.0000\n",
      "5160/5160 [==============================] - 0s 15us/sample - loss: 0.5564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5563639727211738"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding an exponential layer at the output of a regression model can be useful if the values to predict are positive and with very different scales (e.g., 0.001, 10., 10000):\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "    exponential_layer\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=5,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 1.2348 - val_loss: 4.0490\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.5959 - val_loss: 2.0999\n",
      "5160/5160 [==============================] - 0s 16us/sample - loss: 0.4990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49896054055339606"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/my_model_with_a_custom_layer.h5\") # Only works for Sequential or functional APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"models/my_model_with_a_custom_layer.h5\",\n",
    "                                custom_objects={\"MyDense\": MyDense})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return X1 + X2, X1 * X2\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
    "        return [batch_input_shape1, batch_input_shape2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = keras.layers.Input(shape=[2])\n",
    "inputs2 = keras.layers.Input(shape=[2])\n",
    "outputs1, outputs2 = MyMultiLayer()((inputs1, inputs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.4724 - mse: 0.4724 - val_loss: 1.7283 - val_mse: 1.7283\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4258 - mse: 0.4258 - val_loss: 0.8863 - val_mse: 0.8863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2261474db88>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mse\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 16us/sample - loss: 0.4017 - mse: 0.4017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4016558952340784, 0.40165585]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_scaled = X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples\n",
      "Epoch 1/5\n",
      "11610/11610 [==============================] - 1s 99us/sample - loss: 11.8705\n",
      "Epoch 2/5\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 3.7443\n",
      "Epoch 3/5\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 3.6270\n",
      "Epoch 4/5\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 1.7556\n",
      "Epoch 5/5\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 2.8042\n",
      "5160/5160 [==============================] - 0s 32us/sample - loss: 0.9048\n"
     ]
    }
   ],
   "source": [
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"models/my_custom_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples\n",
      "Epoch 1/5\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 1.2797\n",
      "Epoch 2/5\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.9836\n",
      "Epoch 3/5\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 1.5808\n",
      "Epoch 4/5\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.7360\n",
      "Epoch 5/5\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4752\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the above model using the Sequqential API\n",
    "block1 = ResidualBlock(2, 30)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    block1, block1, block1,\n",
    "    ResidualBlock(2, 30),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples\n",
      "Epoch 1/5\n",
      "11610/11610 [==============================] - 1s 98us/sample - loss: 0.9681\n",
      "Epoch 2/5\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4656\n",
      "Epoch 3/5\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.8974\n",
      "Epoch 4/5\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4677\n",
      "Epoch 5/5\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.6956\n",
      "5160/5160 [==============================] - 0s 33us/sample - loss: 0.6339\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        # TODO: check https://github.com/tensorflow/tensorflow/issues/26260\n",
    "        #self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        #if training:\n",
    "        #    result = self.reconstruction_mean(recon_loss)\n",
    "        #    self.add_metric(result)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 109us/sample - loss: 0.8629\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.4169\n"
     ]
    }
   ],
   "source": [
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2]) # Computes the gradient with respect to the variables --> 6w1+2w2, 2w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientTape.gradient can only be called once on non-persistent tapes.\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "try:\n",
    "    dz_dw2 = tape.gradient(z, w2) # Can't call this twice because the tape is automatically erased immediately after its gradient() method is called\n",
    "except RuntimeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients # None because GradientTape() only tracks operations with variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1) # Treat constants like variables\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=136.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "\n",
    "tape.gradient([z1, z2, z3], [w1, w2]) # ---> 6w1+2(w2+i), 2w1, w1=5, w2=3\n",
    "# 136 = 6(5)+2(5) + 6(5)+2(8), 6(5)+2(10) = 40 + 46 + 50\n",
    "# 30 = 2(5)+2(5)+2(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "tape.gradient(z, [w1, w2])  # ---> 6w1+None, None, w1=5, w2=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=nan>]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(100.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x) # softplus = log(1+e^x), gradient is 1/(1+exp(−x)) \n",
    "\n",
    "tape.gradient(z, [x])\n",
    "# Since gradient is just 1/(1+exp(−x)) , why does it return NaN? Due to floating-point precision errors, \n",
    "# autodiff ends up computing infinity divided by infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([inf], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([100.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "z, tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "          end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 - loss: 0.0900 - mean_square: 858.5000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "mean_loss = keras.metrics.Mean(name=\"loss\")\n",
    "mean_square = keras.metrics.Mean(name=\"mean_square\")\n",
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11610/11610 - mean: 1.3323 - mean_absolute_error: 0.9099\n",
      "Epoch 2/5\n",
      "11610/11610 - mean: 1.3708 - mean_absolute_error: 0.9289\n",
      "Epoch 3/5\n",
      "11610/11610 - mean: 1.3456 - mean_absolute_error: 0.9183\n",
      "Epoch 4/5\n",
      "11610/11610 - mean: 1.3148 - mean_absolute_error: 0.9097\n",
      "Epoch 5/5\n",
      "11610/11610 - mean: 1.3109 - mean_absolute_error: 0.9046\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x22617e79688>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube = tf.function(cube)\n",
    "tf_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    print(\"print:\", x)\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print: Tensor(\"x:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "result = tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube.python_function(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
